\section{Notation and Preliminaries}
\label{section:preliminaries}

Let $V$ be a finite-dimensional real vector space equipped with a norm
$\|\cdot\|$. We denote by $V^*$ its dual vector space.  The bi-linear map
associated with $(V^*, V)$ is denoted by $\langle \cdot, \cdot \rangle:V^*
\times V \to \R$.  The dual norm of $\|\cdot\|$ is $\|\cdot\|_*$.

In OLO, in each round $t=1,2,\dots$, the algorithm chooses a point $w_t$ in the
decision set $K \subseteq V$ and then the algorithm observes a loss vector
$\ell_t \in V^*$. The instantaneous loss of the algorithm in round $t$ is
$\langle \ell_t, w_t \rangle$. The cumulative loss of the algorithm after $T$
rounds is $\sum_{t=1}^T \langle \ell_t, w_t \rangle$.  The regret of the
algorithm with respect to a point $u \in K$ is
$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle,
$$
and the regret with respect to the best point is $\Regret_T= \sup_{u \in K}
\Regret_T(u)$.  We assume that $K$ is a non-empty closed convex subset of $V$.
Sometimes we will assume that $K$ is also bounded. We denote by $D$ its
diameter with respect to $\|\cdot\|$, i.e. $D = \sup_{u,v \in K} \|u - v\|$. If
$K$ is unbounded, $D = +\infty$.

\textbf{Convex Analysis.} The \emph{Bregman divergence} of a convex
differentiable function $f$ is defined as $\Breg_f(u,v) = f(u) - f(v) - \langle
\grad f(v), u - v \rangle$.  Note that $\Breg_f(u,v) \ge 0$ for any $u,v$ which
follows directly from the definition of convexity of $f$.

The \emph{Fenchel conjugate} of a function $f:K \to \R$ is the function
$f^*:V^* \to \R \cup \{+\infty\}$ defined as $f^*(\ell) = \sup_{w \in K} \left(
\langle \ell, w \rangle - f(w) \right)$.  The Fenchel conjugate of any function
is convex (since it is a supremum of affine functions) and satisfies for all $w
\in K$ and all $\ell \in V^*$  the \emph{Fenchel-Young inequality}
$
f(w) + f^*(\ell) \ge \langle \ell, w \rangle
$.

Monotonicity of Fenchel conjugates follows easily from the definition: If
$f,g:K \to \R$ satisfy $f(w) \le g(w)$ for all $w \in K$ then $f^*(\ell) \ge
g^*(\ell)$ for every $\ell \in V^*$.

Given $\lambda > 0$, a function $f:K \to \R$ is called \emph{$\lambda$-strongly convex}
with respect to a norm $\|\cdot\|$ if and only if, for all $x,y \in K$,
$$
f(y) \ge f(x) + \langle \grad f(x), y - x \rangle + \frac{\lambda}{2}\|x - y\|^2 \; ,
$$
where $\grad f(x)$ is any subgradient of $f$ at point $x$.

The following proposition relates the range of values of a strongly convex
function to the diameter of its domain. The proof can be found in
Appendix~\ref{section:definitions-proofs}.

\begin{proposition}[Diameter vs. Range]
\label{proposition:diameter-vs-range}
Let $K \subseteq V$ be a non-empty bounded closed convex subset.  Let $D =
\sup_{u,v \in K} \|u - v\|$ be its diameter with respect to $\|\cdot\|$.  Let
$f:K \to \R$ be a non-negative lower semi-continuous function that is
$1$-strongly convex with respect to $\|\cdot\|$.  Then, $D \le \sqrt{8 \sup_{v
\in K} f(v)}$.
\end{proposition}

Fenchel conjugates and strongly convex functions have certain nice properties,
which we list in Proposition~\ref{proposition:conjugate-properties} below.

\begin{proposition}[Fenchel Conjugates of Strongly Convex Functions]
\label{proposition:conjugate-properties}
Let $K \subseteq V$ be a non-empty closed convex set with diameter $D:=\sup_{u,v \in K} \|u-v\|$.
Let $\lambda > 0$, and let $f:K \to \R$ be a lower semi-continuous function
that is $\lambda$-strongly convex with respect to $\|\cdot\|$.
The Fenchel conjugate of $f$ satisfies:
\begin{enumerate}
\item $f^*$ is finite everywhere and differentiable.
\item $\grad f^*(\ell) = \argmin_{w \in K} \left( f(w) - \langle \ell, w \rangle \right)$
\item For any $\ell \in V^*$,
$f^*(\ell) + f(\grad f^*(\ell)) = \langle \ell, \grad f^*(\ell) \rangle$.
\item $f^*$ is $\frac{1}{\lambda}$-strongly smooth i.e. for any $x,y \in V^*$,
$\Breg_{f^*}(x, y) \le \frac{1}{2\lambda} \|x - y\|_*^2$.

\item $f^*$ has $\frac{1}{\lambda}$-Lipschitz continuous gradients i.e.
$\|\grad f^*(x) - \grad f^*(y)\| \le \frac{1}{\lambda} \|x - y\|_*$
for any $x,y \in V^*$.

\item $\Breg_{f^*}(x,y) \le D\|x-y\|_*$ for any $x,y \in V^*$.

\item $\|\grad f^*(x) - \grad f^*(y)\| \le D$ for any $x,y \in V^*$.

\item For any $c > 0$, $(cf(\cdot))^* = cf^*(\cdot/c)$.
\end{enumerate}
\end{proposition}

Except for properties 6 and 7, the proofs can be found
in~\cite{Shalev-Shwartz-2007}.  Property 6 is proven in
Appendix~\ref{section:definitions-proofs}. Property 7 trivially follows from
property 2.


%\label{section:generic-ftrl}

\begin{algorithm}[t]
\caption{\textsc{FTRL with Varying Regularizer}}
\label{algorithm:ftrl-varying-regularizer}
\begin{algorithmic}[1]
\REQUIRE Sequence of regularizers $\{R_t\}_{t=1}^\infty$
\STATE Initialize $L_0 \leftarrow 0$
\FOR{$t=1,2,3,\dots$}
\STATE $w_t \leftarrow \argmin_{w \in K} \left( \langle L_{t-1}, w \rangle + R_t(w) \right)$
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE $L_t \leftarrow L_{t-1} + \ell_t$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Generic FTRL with Varying Regularizer.}
Our scale-free online learning algorithms are versions of the \textsc{Follow
The Regularized Leader} (FTRL) algorithm with varying regularizers, presented
as Algorithm~\ref{algorithm:ftrl-varying-regularizer}.  The following lemma
bounds its regret.

\begin{lemma}[Lemma 1 in \cite{Orabona-Crammer-Cesa-Bianchi-2014}]
\label{lemma:generic-regret-bound}
For any sequence $\{R_t\}_{t=1}^\infty$ of strongly convex lower
semi-continuous regularizers, regret of
Algorithm~\ref{algorithm:ftrl-varying-regularizer} is upper
bounded as
$$
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^{T} \Breg_{R_t^*}(-L_t, -L_{t-1}) - R_t^*(-L_t) + R_{t+1}^*(-L_t) \; .
$$
\end{lemma}
%
The lemma allows data dependent regularizers. That is, $R_t$ can depend on the past loss
vectors $\ell_1, \ell_2, \dots, \ell_{t-1}$.
