\section{Notation and Preliminaries}
\label{section:preliminaries}

Let $V$ be a finite-dimensional\footnote{Many, but not all, of our
results can be extended to more general normed vector spaces.} real vector
space equipped with a norm $\|\cdot\|$. We denote by $V^*$ its dual vector
space.  The bi-linear map associated with $(V^*, V)$ is denoted by $\langle
\cdot, \cdot \rangle:V^* \times V \to \R$.  The dual norm of $\|\cdot\|$ is
$\|\cdot\|_*$.

In OLO, in each round $t=1,2,\dots$, the algorithm chooses a point $w_t$ in the
decision set $K \subseteq V$ and then the algorithm observes a loss vector
$\ell_t \in V^*$. The instantaneous loss of the algorithm in round $t$ is
$\langle \ell_t, w_t \rangle$. The cumulative loss of the algorithm after $T$
rounds is $\sum_{t=1}^T \langle \ell_t, w_t \rangle$.  The regret of the
algorithm with respect to a point $u \in K$ is
$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle,
$$
and the regret with respect to the best point is $\Regret_T= \sup_{u \in K}
\Regret_T(u)$.  We assume that $K$ is a non-empty closed convex subset of $V$.
Sometimes we will assume that $K$ is also bounded. We denote by $D$ its
diameter with respect to $\|\cdot\|$, i.e. $D = \sup_{u,v \in K} \|u - v\|$. If
$K$ is unbounded, $D = +\infty$.

\subsection{Convex Analysis}

The \emph{Bregman divergence} of a convex differentiable function $f$ is
defined as $\Breg_f(u,v) = f(u) - f(v) - \langle \grad f(v), u - v \rangle$.
Note that $\Breg_f(u,v) \ge 0$ for any $u,v$ which follows directly from the
definition of convexity of $f$.

The \emph{Fenchel conjugate} of a function $f:K \to \R$ is the function
$f^*:V^* \to \R \cup \{+\infty\}$ defined as $f^*(\ell) = \sup_{w \in K} \left(
\langle \ell, w \rangle - f(w) \right)$.  The Fenchel conjugate of any function
is convex (since it is a supremum of affine functions) and satisfies
 the \emph{Fenchel-Young inequality}
$$
\text{$\forall w \in K$, $\forall \ell \in V^*$} \qquad
f(w) + f^*(\ell) \ge \langle \ell, w \rangle \; .
$$

Monotonicity of Fenchel conjugates follows easily from the definition: If
$f,g:K \to \R$ satisfy $f(w) \le g(w)$ for all $w \in K$ then $f^*(\ell) \ge
g^*(\ell)$ for every $\ell \in V^*$.

Given $\lambda > 0$, a function $f:K \to \R$ is called \emph{$\lambda$-strongly
convex} with respect to a norm $\|\cdot\|$ if and only if, for all $x,y \in K$,
$$
f(y) \ge f(x) + \langle \grad f(x), y - x \rangle + \frac{\lambda}{2}\|x - y\|^2 \; ,
$$
where $\grad f(x)$ is any subgradient of $f$ at point $x$.

The following proposition relates the range of values of a strongly convex
function to the diameter of its domain. The proof can be found
in~\ref{section:definitions-proofs}.

\begin{proposition}[Diameter vs. Range]
\label{proposition:diameter-vs-range}
Let $K \subseteq V$ be a non-empty bounded closed convex subset.  Let $D =
\sup_{u,v \in K} \|u - v\|$ be its diameter with respect to $\|\cdot\|$.  Let
$f:K \to \R$ be a non-negative lower semi-continuous function that is
$1$-strongly convex with respect to $\|\cdot\|$.  Then, $D \le \sqrt{8 \sup_{v
\in K} f(v)}$.
\end{proposition}

Fenchel conjugates and strongly convex functions have certain nice properties,
which we list in Proposition~\ref{proposition:conjugate-properties} below.

\begin{proposition}[Fenchel Conjugates of Strongly Convex Functions]
\label{proposition:conjugate-properties}
Let $K \subseteq V$ be a non-empty closed convex set with diameter
$D:=\sup_{u,v \in K} \|u-v\|$.  Let $\lambda > 0$, and let $f:K \to \R$ be a
lower semi-continuous function that is $\lambda$-strongly convex with respect
to $\|\cdot\|$.  The Fenchel conjugate of $f$ satisfies:
\begin{enumerate}

\item $f^*$ is finite everywhere and differentiable.

\item For any $\ell \in V^*$, $\grad f^*(\ell) = \argmin_{w \in K} \left( f(w) - \langle \ell, w \rangle \right)$

\item For any $\ell \in V^*$, $f^*(\ell) + f(\grad f^*(\ell)) = \langle \ell, \grad f^*(\ell) \rangle$.

\item $f^*$ is $\frac{1}{\lambda}$-strongly smooth i.e. for any $x,y \in V^*$, $\Breg_{f^*}(x, y) \le \frac{1}{2\lambda} \|x - y\|_*^2$.

\item $f^*$ has $\frac{1}{\lambda}$-Lipschitz continuous gradients i.e.
for any $x,y \in V^*$,
$\|\grad f^*(x) - \grad f^*(y)\| \le \frac{1}{\lambda} \|x - y\|_*$.

\item $\Breg_{f^*}(x,y) \le D\|x-y\|_*$ for any $x,y \in V^*$.

\item $\|\grad f^*(x) - \grad f^*(y)\| \le D$ for any $x,y \in V^*$.

\item For any $c > 0$, $(cf(\cdot))^* = cf^*(\cdot/c)$.
\end{enumerate}
\end{proposition}

Except for properties 6 and 7, the proofs can be found
in~\cite{Shalev-Shwartz-2007}.  Property 6 is proven
in~\ref{section:definitions-proofs}. Property 7 trivially follows from property
2.

\begin{algorithm}[t]
\caption{\textsc{FTRL with Varying Regularizer}}
\label{algorithm:ftrl-varying-regularizer}
\begin{algorithmic}[1]
\REQUIRE Non-empty closed convex set $K \subseteq V$
\STATE Initialize $L_0 \leftarrow 0$
\FOR{$t=1,2,3,\dots$}
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_t \leftarrow \argmin_{w \in K} \left( \langle L_{t-1}, w \rangle + R_t(w) \right)$
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE $L_t \leftarrow L_{t-1} + \ell_t$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Generic FTRL with Varying Regularizer}
\label{section:generic-ftrl}

Our scale-free online learning algorithms are versions of the \textsc{Follow
The Regularized Leader}\footnote{The name \textsc{Follow The Regularized
Leader} comes from~\cite{Abernethy-Hazan-Rakhlin-2008}. It dates back to the potential-based forecaster in \cite[Chapters~11]{Cesa-Bianchi-Lugosi-2006} and its theory was heavily developed by \cite{Shalev-Shwartz-2007}. 
Independently, the same algorithm was proposed by~\cite{Nestorov-2009} for convex optimization under the name \textsc{Dual
Averaging} and later rediscovered in~\cite{Xiao-2010} for online convex optimization. The version with time-varying regularizers was analyzed in \cite{Orabona-Crammer-Cesa-Bianchi-2014}.
} (\textsc{FTRL})
algorithm with \emph{varying regularizers}, presented as
Algorithm~\ref{algorithm:ftrl-varying-regularizer}. The algorithm is
paramatrized by a sequence $\{R_t\}_{t=1}^\infty$ of convex functions $R_t:K
\to \R$ called \emph{regularizers}.  Each regularizer $R_t$ can depend on the
past loss vectors $\ell_1, \ell_2, \dots, \ell_{t-1}$ in an arbitrary way.  The
following lemma bounds its regret.

\begin{lemma}[Regret of Follow The Regularized Leader]
\label{lemma:generic-regret-bound}
For any sequence $\{R_t\}_{t=1}^\infty$ of strongly convex lower
semi-continuous regularizers, the regret of
Algorithm~\ref{algorithm:ftrl-varying-regularizer} is upper
bounded as
$$
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^{T} \Breg_{R_t^*}(-L_t, -L_{t-1}) - R_t^*(-L_t) + R_{t+1}^*(-L_t) \; .
$$
\end{lemma}

The proof of the lemma can be found in~\cite{Orabona-Crammer-Cesa-Bianchi-2014}.
For completeness, we include it in~\ref{section:definitions-proofs}.
