\section{Conclusions}
\label{section:conclusions}

We have investigated scale-free algorithms for online linear optimization and
we have shown that scale-free property leads to algorithms which have optimal
regret and do not need to know or assume \textbf{anything} about the sequence
of loss vectors, i.e. without any assumption on the norms of the loss vectors
and the number of rounds. We have designed two scale-free algorithms for online linear
optimization with regret bound $O \left(\sqrt{\sup_{v \in K} f(v) \sum_{t=1}^T
\|\ell\|_*^2} \right)$ where $f$ is any non-negative $1$-strongly convex
function with respect to a norm $\|\cdot\|$ defined on the decision set and
where $\|\cdot\|_*$ is the dual norm to $\|\cdot\|$.  These algorithms are
based on \textsc{Follow The Regularizer Leader}.  We have also designed an
algorithm with regret $O\left((R(u) + \frac{1}{\lambda}) \sqrt{T}
\max_{t=1,2,\dots,T} \|\ell_t\|_* \right)$ with respect to any competitor $u
\in K$, where $R$ is any non-negative $\lambda$-strongly convex function
defined on the decision set.  The latter result makes sense even when the
decision set $K$ is unbounded.

Similar, but weaker result holds for a scale-free algorithm based on
\textsc{Mirror Descent}. However, we have also shown this algorithm is strictly
weaker than algorithms based on \textsc{Follow The Regularizer Leader}. Namely,
we gave examples of regularizers for which the associated Bregman divergence is
unounded and the scale-free version of \textsc{Mirror Descent} has $\Omega(T)$
regret or worse.

We have shown an $\Omega\left( D \sqrt{\sum_{t=1}^T \|\ell\|_*^2}\right)$
lower bound on regret of any algorithm for decision set with diameter $D$.
