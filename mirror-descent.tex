\section{Mirror Descent}
\label{section:mirror-descent}

\begin{algorithm}[t]
\caption{\textsc{Mirror Descent with Varying Regularizer}}
\label{algorithm:mirror-descent-varying-regularizer}
\begin{algorithmic}[1]
\REQUIRE Non-empty closed convex set $K \subseteq V$
\STATE Choose a regularizer $R_0:K \to \R$
\STATE $w_1 \leftarrow \argmin_{w \in K} R_0(w)$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_{t+1} \leftarrow \argmin_{w \in K} \left( \langle \ell, w \rangle + \Breg_{R_t}(w, w_t) \right)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\textsc{Mirror Descent} is a generic algorithm similar to \textsc{Follow The
Regularized Leader} but quite different in details. The algorithm is stated as
Algorithm~\ref{algorithm:mirror-descent-varying-regularizer}. The algorithm is
parametrized by a sequence $\{R_t\}_{t=0}^\infty$ of convex functions $R_t:K
\to \R$ called \emph{regularizers}. Each regularizer $R_t$ can depend on
past loss vectors $\ell_1, \ell_2, \dots, \ell_t$ in an
arbitrary way. If $R_t$ is not differentiable\footnote{Note that this can
happen even when $R_t$ is a restriction of a differentiable function defined on
a superset of $K$.  If $K$ is bounded and closed, $R_t$ fails to be
differentiable at the boundary of $K$. If $K$ is a subset of an affine subspace
of a dimension smaller than the dimension of $V$, then $R_t$ fails to be
differentiable everywhere.}, to define Bregman divergence, $\Breg_{R_t}(u,v) = R(u) - R(v) - \langle \grad R(v), u - v \rangle$, the
algorithm has to choose together with $R_t$ a subgradient map $\grad R_t:K \to
V$.  If $R_t$ is a restriction of a differentiable function $R'_t$, it is convenient
to define $\grad R_t(w) = \grad R'_t(w)$ for all $w \in K$. The following lemma
bounds instantaneous regret of \textsc{Mirror Descent}.

\begin{lemma}[Regret of Mirror Descent]
Algorithm~\ref{algorithm:mirror-descent-varying-regularizer} for any $u \in K$ satisfies
$$
\Regret_T(u) \le \sum_{t=1}^T \langle \ell_t, w_t - w_{t+1} \rangle - \Breg_{R_t}(w_{t+1}, w_t) + \Breg_{R_t}(u,w_t) - \Breg_{R_t}(u, w_{t+1}) \; .
$$
\end{lemma}

Several existing algorithms are special cases of \textsc{Mirror Descent}. Most
prominent example is projected gradient descent defined by $w_{t+1} = \Pi_K(w_t
- \eta_t \ell_t)$ where $\Pi_K$ is the Euclidean projection to $K$ and
$\eta_t>0$ is the step size.  Projected gradient descent is an instance of
\textsc{Mirror Descent} with regularizer $R_t(w) = \frac{1}{2\eta_t}\|w\|_2^2$.
A special case of projected gradient descent with step size $\eta_t = \frac{1}{\sqrt{t}}$
is Zinkevich's \textsc{Generalized Infinitesimal Gradient Ascent} (GIGA)
algorithm~\cite{Zinkevich-2003}. Full-matrix and diagonal versions of \textsc{AdaGrad with Composite Mirror
Descent Update}~\cite{Duchi-Hazan-Singer-2011} are an instances of
\textsc{Mirror Descent} for any bounded $K$ with regularizers
$$
R_t(w) = \frac{1}{2} w^\top \left(\sum_{i=1}^t \ell_i \ell_i^\top \right)^{1/2} \!\!\!\! w
\qquad \text{and} \qquad
R_t(w) = \frac{1}{2} \sum_{j=1}^d w_j^2 \sqrt{ \sum_{i=1}^t \ell_{i,j}^2} \; .
$$
respectively. When $d=1$, both algorithms are equivalent to
\begin{equation}
\label{equation:ada-grad-regularizer}
R_t(w) = \frac{\|w\|_2^2}{2} \sqrt{\sum_{i=1}^t \|\ell_i\|_2^2 } \; .
\end{equation}
All three sequences of regularizers give a scale-free algorithm.
Also note that regularizer \eqref{equation:ada-grad} can be viewed
as a scale-free generalization of Zinkevich's GIGA algorithm; in particular, when $\|\ell_i\|_2 = 1$ for all $i$,
\textsc{Mirror Descent} with regularizer \eqref{equation:ada-grad-regularizer} coincides with GIGA.

\textsc{Exponentially-Weighted Average Forecaster}~\cite[Chapter
2]{Cesa-Bianchi-Lugosi-2006} with constant learning rate $\eta$ for the problem
of prediction with expert advice is an instance of \textsc{Mirror Descent} for
the probability simplex $K = \{ w \in \R^d ~:~ w_i \ge 0, \sum_{i=1}^d w_i = 0
\}$ with negative entropy regularizer $R_t(w) = \frac{1}{\eta} \sum_{i=1}^d w_i
\ln w_i$ where $R_t:K \to \R$. Note that this algorithm is \emph{not} scale-free.

Replacing $\frac{1}{2}\|w\|_2^2$ in equation $\label{equation:ada-grad}$
with an arbitrary convex function $R:K \to \R$ yields
\begin{equation}
\label{equation:scale-free-mirror-descent}
R_t(w) = R(w) \sqrt{\sum_{i=1}^t \|\ell_i\|_*^2} \; .
\end{equation}
We call the resulting algorithm \textsc{Scale-Free Mirror Descent} with regularizer $R$.
Generalizing existing regret bound for
\textsc{AdaGrad}~\cite{Duchi-Hazan-Singer-2011} we can upper bound its regret.

\begin{theorem}[Regret of Scale-Free Mirror Descent]
\label{theorem:regret-scale-free-mirror-descent}
Suppose $K \subseteq V$ is a non-empty closed convex subset. Suppose
that $R:K \to \R$ is a $\lambda$-strongly convex function with respect to a norm $\|\cdot\|$.
\textsc{Scale-Free Mirror Descent} with regularizer $R$ satisfies
$$
\Regret_T(u) \le \left( \frac{1}{\lambda} + \sup_{v \in K} \Breg_R(u,v) \right) \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} \; .
$$
\end{theorem}

We choose regularizer $R(w) = \lambda f(w)$ where $f$ is a $1$-strongly convex function
and optimize $\lambda$.

\begin{corollary}[Regret of Scale-Free Mirror Descent]
\label{corollary:regret-scale-free-mirror-descent}
Suppose $K \subseteq V$ is a non-empty bounded closed convex subset.  Suppose
that $f:K \to \R$ is a $1$-strongly convex function with respect to a norm $\|\cdot\|$.
\textsc{Scale-Free Mirror Descent} with regularizer
$$
R(w) = \frac{f(w)}{\displaystyle \sqrt{\sup_{u,v \in K} \Breg_f(u,v)}}
\quad \text{satisfies} \quad %
\Regret_T(u) \le 2 \sqrt{\sup_{u,v \in K} \Breg_f(u,v) \sum_{t=1}^T \|\ell_t\|_*^2} \; .
$$
\end{corollary}

\subsection{Lower Bounds}

The bounds in Theorem~\ref{theorem:regret-scale-free-mirror-descent} and
Corollary~\ref{corollary:regret-scale-free-mirror-descent} are vacuous when
$\Breg_R(u,v)$ is not bounded. One might wonder if the
assumption that $\Breg_R(u,v)$ is bounded is necessary. We show necessity of
this assumption on two counter-examples where $\Breg_R(u,v)$ is not bounded.
In these counter-examples, we construct sequences of loss vectors $\ell_1,
\ell_2, \dots, \ell_T$ such that $\|\ell_1\|_* = \|\ell_2\|_* = \dots =
\|\ell_T\|_* = 1$ and \textsc{Scale-Free Mirror Descent} has $\Omega(T)$
or worse regret.

The first counter-example is stated as
Theorem~\ref{theorem:first-counter-example} below. The decision set is the
whole space $K=V$ and the regularizer is $R(w) = \frac{1}{2}\|w\|_2^2$. Note
that $R(w)$ is $1$-strongly convex with respect to $\|\cdot\|_2$ and the dual
norm of $\|\cdot\|_2$ is $\|\cdot\|_2$. Corresponding Bregman divergence is
$\Breg_R(u,v) = \frac{1}{2}\|u-v\|_2^2$. The counter-example constructs
sequence of unit-norm loss vectors in the one-dimensional subspace spanned by
the first vector of the standard unit basis.  On such sequence, Zinkevich's
\textsc{GIGA} and both versions of \textsc{AdaGrad with Composite Mirror Update} have
predictions identical to \textsc{Scale-Free Mirror Descent}. Hence the lower
bound applies to all four algorithms.

\begin{theorem}[First Counter-Example]
\label{theorem:first-counter-example}
Suppose $K = V$. For any $T \ge 42$ there exists a sequence of loss vectors $\ell_1, \ell_2, \dots, \ell_T \in V^*$
such that $\|\ell_1\|_2 = \|\ell_2\|_2 = \dots = \|\ell_T\|_2 = 1$ and \textsc{Scale-Free Mirror Descent}
with regularizer $R(w) = \frac{1}{2}\|w\|_w^2$, Zinkevich's \textsc{GIGA} algorithm, and
both versions of \textsc{AdaGrad with Composite Mirror Descent Update} satisfy
$$
\Regret_T(0) \ge \frac{T^{3/2}}{20} \; .
$$
\end{theorem}

The second counter-example is as Theorem~\ref{theorem:second-counter-example}
below.  The decision set is the $d$-dimensional probability simplex $K = \{ w \in \R^d ~:~ w_i
\ge 0, \sum_{i=1}^d w_i = 0 \}$ and the regularizer is the negative entropy
$R(w) = \sum_{i=1}^d w_i \ln w_i$.  Negative entropy is $1$-strongly convex
with respect to $\|\cdot\|_1$ and the dual norm of $\|\cdot\|_1$ is
\mbox{$\|\cdot\|_\infty$}.  The corresponding Bregman divergence is the
Kullback-Leibler divergence $\Breg_R(u,v) = \sum_{i=1}^d u_i \ln(u_i/v_i)$.
Note that despite that negative entropy is upper- and lower-bounded,
Kullback-Leibler divergence is not.

\begin{theorem}[Second Counter-Example]
\label{theorem:second-counter-example}
Let $d \ge 2$ and let $K = \{ w \in \R^d ~:~ w_i \ge 0, \sum_{i=1}^d w_i = 0 \} \subseteq V$ be the $d$-dimensional
probability simplex.
For any $T \ge 120$ there exists a sequence of loss vectors $\ell_1, \ell_2, \dots, \ell_T \in V^*$
such that $\|\ell_1\|_\infty = \|\ell_2\|_\infty = \dots = \|\ell_T\|_\infty = 1$ and \textsc{Scale-Free Mirror Descent}
with regularizer $R(w) = \sum_{i=1}^d w_i \ln w_i$ satisfies
$$
\Regret_T \ge \frac{T}{6} \; .
$$
\end{theorem}


