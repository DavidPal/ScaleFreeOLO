\section{\textsc{AdaFTRL}}
\label{section:ada-ftrl}

In this section we generalize the \textsc{AdaHedge}
algorithm~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} to the OLO setting,
showing that it retains its scale-free property. The analysis is very general
and based on general properties of strongly convex functions, rather than
specific properties of the entropic regularizer like in \textsc{AdaHedge}.

Assume that $K$ is bounded and that $R:K \to \R$ is a strongly convex lower
semi-continuous function bounded from above.  We instantiate
Algorithm~\ref{algorithm:ftrl-varying-regularizer} with the sequence of
regularizers
\begin{equation}
\label{equation:ada-ftrl}
R_t(w) = \Delta_{t-1} R(w)
\quad \text{where}
\quad \Delta_{t}=\sum_{i=1}^{t} \Delta_{i-1} \Breg_{R^*}\left(- \frac{L_i}{\Delta_{i-1}}, -\frac{L_{i-1}}{\Delta_{i-1}}\right) \; .
\end{equation}

The sequence $\{\Delta_t\}_{t=0}^\infty$ is non-negative and non-decreasing.
Also, $\Delta_t$ as a function of $\ell_1, \ell_2, \dots, \ell_t$ is positive
homogeneous of degree one, making the algorithm scale-free.

If $\Delta_{i-1} = 0$, we define $\Delta_{i-1}
\Breg_{R^*}(\frac{-L_i}{\Delta_{i-1}}, \frac{-L_{i-1}}{\Delta_{i-1}})$ as
$\lim_{a \to 0^+} a \Breg_{R^*}(\frac{-L_i}{a}, \frac{-L_{i-1}}{a})$ which
always exists and is finite; see Lemma~\ref{lemma:limit-bregman-divergence}
in~\ref{section:limits}.  Similarly, when $\Delta_{t-1} = 0$, we define $w_t =
\argmin_{w \in K} \langle L_{t-1}, w \rangle$ where ties among minimizers are
broken by taking the one with the smallest value of $R(w)$, which is unique due
to strong convexity. As we show in Lemma~\ref{lemma:prediction-limit-existence}
in~\ref{section:limits}, this is the same as $w_t = \lim_{a \to 0^+} \argmin_{w
\in K} (\langle L_{t-1}, w \rangle + aR(w))$.

Our main result is an $O(\sqrt{\sum_{t=1}^T \|\ell_t\|_*^2})$ upper bound on
the regret of the algorithm after $T$ rounds, without the need to know before
hand an upper bound on $\|\ell_t\|_*$.  We prove the theorem in
Section~\ref{section:ada-ftrl-regret-bound}.

\begin{theorem}[Regret Bound]
\label{theorem:ada-ftrl-regret-bound}
Suppose $K \subseteq V$ is a non-empty bounded closed convex set. Let $D =
\sup_{x,y \in K} \|x - y\|$ be its diameter with respect to a norm $\|\cdot\|$.
Suppose that the regularizer $R:K \to \R$ is a non-negative lower
semi-continuous function that is $\lambda$-strongly convex with respect to
$\|\cdot\|$ and is bounded from above.  The regret of \textsc{AdaFTRL}
satisfies
$$
\Regret_T(u)
\le \sqrt{3} \max\left\{D, \frac{1}{\sqrt{2\lambda}} \right\}
    \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} \left(1 + R(u) \right) \; .
$$
\end{theorem}

The regret bound can be optimized by choosing the optimal multiple of the
regularizer.  Namely, we choose regularizer of the form $\lambda f(w)$ where
$f(w)$ is $1$-strongly convex and optimize over $\lambda$. The result of the
optimization is the following corollary.  Its proof can be found
in~\ref{section:ada-ftrl-proofs}.

\begin{corollary}[Regret Bound]
\label{corollary:ada-ftrl-regret-bound}
Suppose $K \subseteq V$ is a non-empty bounded closed convex set. Suppose $f:K
\to \R$ is a non-negative lower semi-continuous function that is $1$-strongly
convex with respect to $\|\cdot\|$ and is bounded from above.  The regret of
\textsc{AdaFTRL} with regularizer
$$
R(w) = \frac{f(w)}{16 \cdot \sup_{v \in K} f(v)}
\qquad \text{satisfies} \qquad
\Regret_T \le
5.3 \sqrt{\sup_{v \in K} f(v) \sum_{t=1}^T \|\ell_t\|_*^2} \; .
$$
\end{corollary}

\subsection{Proof of Regret Bound for \textsc{AdaFTRL}}
\label{section:ada-ftrl-regret-bound}

\begin{lemma}[Initial Regret Bound]
\label{lemma:initial-regret-bound}
\textsc{AdaFTRL} satisfies, for any $u \in K$ and any $T \ge 0$,
$$
\Regret_T(u) \le \left(1 + R(u) \right) \Delta_T \; .
$$
\end{lemma}

\begin{proof}
Let $R_t(w) = \Delta_{t-1} R(w)$. Since $R$ is non-negative,
$\{R_t\}_{t=1}^\infty$ is non-decreasing.  Hence, $R_t^*(\ell) \ge
R_{t+1}^*(\ell)$ for every $\ell \in V^*$ and thus $R_t^*(-L_t) -
R_{t+1}^*(-L_t) \ge 0$.  So, by Lemma~\ref{lemma:generic-regret-bound},
\begin{equation}
\label{equation:regret-bound-inequality}
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^{T} \Breg_{R_t^*}(-L_t, -L_{t-1}) \; .
\end{equation}
Technically, \eqref{equation:regret-bound-inequality} is not justified since
$R_t$ might not be strongly convex. This happens when $\Delta_{t-1} = 0$. In
order to justify \eqref{equation:regret-bound-inequality}, we consider a
different algorithm that initializes $\Delta_0 = \epsilon$ where $\epsilon >
0$; that ensures that $\Delta_{t-1} > 0$ and $R_t$ is strongly convex.
Applying  Lemma~\ref{lemma:generic-regret-bound} and then taking limit
$\epsilon \to 0$, yields \eqref{equation:regret-bound-inequality}.

Since, $\Breg_{R_t^*}(u,v) = \Delta_{t-1} \Breg_{R^*}(\frac{u}{\Delta_{t-1}},
\frac{v}{\Delta_{t-1}})$ by definition of Bregman divergence and property 8 of
Proposition~\ref{proposition:conjugate-properties}, we have $\sum_{t=1}^T
\Breg_{R_t^*}(-L_t, -L_{t-1}) = \Delta_T$.
\end{proof}

\begin{lemma}[Recurrence]
\label{lemma:gap-recurrence}
Let $D = \sup_{u, v \in K} \|u -v\|$ be the diameter of $K$.  The sequence
$\{\Delta_t\}_{t=1}^\infty$ generated by \textsc{AdaFTRL} satisfies for any $t
\ge 1$,
$$
\Delta_t \le \Delta_{t-1} + \min \left\{ D\|\ell_t\|_*, \ \frac{\|\ell_t\|_*^2}{2\lambda \Delta_{t-1}} \right\} \; .
$$
\end{lemma}

\begin{proof}
By definition, $\Delta_t$ satisfies the recurrence
$$
\Delta_t = \Delta_{t-1} + \Delta_{t-1} \Breg_{R^*}\left( - \frac{L_t}{\Delta_{t-1}}, - \frac{L_{t-1}}{\Delta_{t-1}} \right) \; .
$$
Using parts 4 and 6 of Proposition~\ref{proposition:conjugate-properties}, we
can upper bound $\Breg_{R^*}\left( - \frac{L_t}{\Delta_{t-1}}, -
\frac{L_{t-1}}{\Delta_{t-1}} \right)$ with two different quantities.  Taking
minimum of the two quantities finishes the proof.
\end{proof}

The recurrence of Lemma~\ref{lemma:gap-recurrence} can be simplified. Defining
$$
a_t = \|\ell_t\|_* \max \left\{D, \frac{1}{\sqrt{2 \lambda}} \right\} \; ,
$$
we get a recurrence
$$
\Delta_t \le \Delta_{t-1} + \min \left\{ a_t, \ \frac{a_t^2}{\Delta_{t-1}} \right\} \; .
$$
The next lemma solves this recurrence, by giving an explicit upper bound on
$\Delta_T$ in terms of $a_1, a_2, \dots, a_T$.

\begin{lemma}[Solution of the Recurrence]
\label{lemma:recurrence-solution}
Let $\{a_t\}_{t=1}^\infty$ be any sequence of non-negative real numbers.
Suppose that $\{\Delta_t\}_{t=0}^\infty$ is a sequence of real numbers
satisfying
$$
\Delta_0 = 0 \qquad \text{and} \qquad
\Delta_t \le \Delta_{t-1} + \min \left\{ a_t, \ \frac{a_t^2}{\Delta_{t-1}} \right\} \quad \text{for any $t \ge 1$} \; .
$$
Then, for any $T \ge 0$,
$$
\Delta_T \le \sqrt{3 \sum_{t=1}^T a_t^2} \; .
$$
\end{lemma}
%
Proof of the Lemma~\ref{lemma:recurrence-solution} is deferred
to~\ref{section:ada-ftrl-proofs}.  Theorem~\ref{theorem:ada-ftrl-regret-bound}
follows from
Lemmas~\ref{lemma:initial-regret-bound},~\ref{lemma:gap-recurrence}~and~\ref{lemma:recurrence-solution}.
