\section{Introduction}
\label{section:introduction}

Online Linear Optimization (OLO) is a problem where an algorithm repeatedly
chooses a point $w_t$ from a convex decision set $K$, observes an arbitrary, or
even adversarially chosen, loss vector $\ell_t$ and suffers the loss $\langle
\ell_t, w_t \rangle$.  The goal of the algorithm is to have a small cumulative
loss. The performance of an algorithm is evaluated by the so-called regret,
which is the difference of the cumulative losses of the algorithm and of the
(hypothetical) strategy that would choose in every round the same best point in
hindsight.

OLO is a fundamental problem in machine
learning~\cite{Cesa-Bianchi-Lugosi-2006, Rakhlin-Sridharan-2009,
Shalev-Shwartz-2011}.  Many learning problems can be directly phrased as OLO,
e.g., learning with expert advice~\cite{Littlestone-Warmuth-1994, Vovk-1998,
Freund-Schapire-1997, Cesa-Bianchi-Haussler-Helmbold-Schapire-Warmuth-1997},
online combinatorial optimization~\cite{Kalai-Vempala-2005,
Helmbold-Warmuth-2009, Koolen-Warmuth-Kivinen-2010}. Other problems can be
reduced to OLO, e.g. online convex
optimization~\cite{Abernethy-Bartlett-Rakhlin-Tewari-2008},
\cite[Chapter~2]{Shalev-Shwartz-2011}, online classification
~\cite{Rosenblatt-1958,Freund-Schapire-1999} and
regression~\cite{Kivinen-Warmuth-1997},
~\cite[Chapters~11~and~12]{Cesa-Bianchi-Lugosi-2006}, multi-armed
problems~\cite[Chapter~6]{Cesa-Bianchi-Lugosi-2006},
\cite{Abernethy-Hazan-Rakhlin-2008, Bubeck-Cesa-Bianchi-2012}, and batch
and stochastic optimization of convex functions~\cite{Nemirovski-Yudin-1983,
Bubeck-2015}.  Hence, a result in OLO immediately implies other results in all
these domains.

The adversarial choice of the loss vectors received by the algorithm is what
makes the OLO problem challenging. In particular, if an OLO algorithm commits
to an upper bound on the norm of future loss vectors, its regret can be made
arbitrarily large through an adversarial strategy that produces loss vectors
with norms that exceed the upper bound.

For this reason, most of the existing OLO algorithms receive as an input---or
explicitly assume---an upper bound $B$ on the norm of the loss vectors.  The
input $B$ is often disguised as the learning rate, the regularization
parameter, or the parameter of strong convexity of the regularizer. Examples of
such algorithms include the \textsc{Hedge} algorithm or online projected
gradient descent with fixed learning rate.  However, these algorithms have two
obvious drawbacks.

First, they do not come with any regret guarantee for sequences of loss vectors
with norms exceeding $B$. Second, on sequences where the norm of loss vectors
is bounded by $b \ll B$, these algorithms fail to have an optimal regret
guarantee that depends on $b$ rather than on $B$.

% Change spacing between rows.
\renewcommand{\arraystretch}{1.8}

\begin{table}[t]
\fontsize{8}{8.2}\selectfont
\centering
\begin{tabular}{|p{3.6cm}|c|p{3.4cm}|c|}
\hline
\textbf{Algorithm} & \textbf{Decisions Set(s)} & \textbf{Regularizer(s)} & \textbf{Scale-Free} \\ \hline \hline
\textsc{Hedge} \cite{Freund-Schapire-1997} & Probability Simplex & Negative Entropy & No \\ \hline
\textsc{GIGA} \cite{Zinkevich-2003} & Any Bounded & $\frac{1}{2}\|w\|_2^2$ & No \\ \hline
\textsc{RDA} \cite{Xiao-2010} & \textbf{Any} & \textbf{Any Strongly Convex} & No \\ \hline
\textsc{FTRL-Proximal} \cite{McMahan-Streeter-2010,McMahan-2014} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & \textbf{Yes} \\ \hline
\textsc{AdaGrad MD} \cite{Duchi-Hazan-Singer-2011} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & \textbf{Yes} \\ \hline
\textsc{AdaGrad FTRL} \cite{Duchi-Hazan-Singer-2011} & \textbf{Any} & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & No \\ \hline
\textsc{AdaHedge} \cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} & Probability Simplex & Negative Entropy & \textbf{Yes} \\ \hline
\textsc{Optimistic MD} \cite{Rakhlin-Sridharan-2013} & $\sup_{u,v \in K} \Breg_f(u,v) < \infty$ & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{NAG} \cite{Ross-Mineiro-Langford-2013} & $\{u: \max_t \langle \ell_t, u\rangle \le C\}$ & $\frac{1}{2}\|w\|_2^2 $& Partially\footnotemark \\ \hline
\textsc{Scale invariant algorithms} \cite{Orabona-Crammer-Cesa-Bianchi-2014} & \textbf{Any} & $\frac{1}{2}\|w\|_p^2 + $ any convex func. \newline $1 < p \le 2$ & Partially\textsuperscript{\ref{footnote-label}} \\ \hline
\textsc{AdaFTRL} \textbf{[this paper]} & Any Bounded & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{SOLO FTRL} \textbf{[this paper]} & \textbf{Any} & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\end{tabular}
\caption{Selected results for OLO. Best results in each column are in bold.
\label{table:results}
}
\end{table}

\footnotetext{\label{footnote-label} These algorithms attempt to produce an
invariant sequence of predictions $\langle w_t, \ell_t \rangle$, rather than a
sequence of invariant $w_t$.}

There is a clear practical need to design algorithms that adapt automatically
to norms of the loss vectors.  A natural, yet overlooked, design method to
achieve this type of adaptivity is by insisting to have a \textbf{scale-free}
algorithm.  That is, the sequence of decisions of the algorithm does not change
if the sequence of loss vectors is multiplied by a positive constant.

% \begin{definition}[Scale-Free Algorithm]
% \label{definition:scale-free-algorithm}
% An algorithm for OLO is \emph{scale-free} if for any sequence of loss vectors
% $\{\ell_t\}_{t=1}^\infty$ and any real number $c > 0$, on the sequences of
% $\{\ell_t\}_{t=1}^\infty$ and $\{c \cdot \ell_t\}_{t=1}^\infty$, the algorithm
% chooses the same sequence of points $\{w_t\}_{t=1}^\infty$.
% \end{definition}

The most important property of scale-free algorithms is that both their loss
and their regret is scales linearly with the maximum norm of the loss vector
appearing in the sequence.

\subsection{Previous results}

Majority of existing algorithms for OLO are based on two generic algorithms:
\textsc{Follow The Regularizer Leader} (\textsc{FTRL}) and \textsc{Mirror
Descent} (\textsc{MD}). \textsc{FTRL} dates back to the potential-based
forecaster in \cite[Chapters~11]{Cesa-Bianchi-Lugosi-2006} and its theory was
developed in \cite{Shalev-Shwartz-2007}.  The name \textsc{Follow The
Regularized Leader} comes from~\cite{Abernethy-Hazan-Rakhlin-2008}.
Independently, the same algorithm was proposed in~\cite{Nestorov-2009} for
convex optimization under the name \textsc{Dual Averaging} and later
rediscovered in~\cite{Xiao-2010} for online convex optimization. The general
version with time-varying regularizers was analyzed in
\cite{Orabona-Crammer-Cesa-Bianchi-2014}. \textsc{MD} was originally proposed
in \cite{Nemirovski-Yudin-1983} and later analyzed in~\cite{Beck-Teboulle-2003}
for convex optimization. In the online learning literature it makes the first
appearance, under a different name, in \cite{Kivinen-Warmuth-1997}. Good
description of \textsc{MD} is given in~\cite{Rakhlin-Sridharan-2009}.

Both \textsc{FTRL} and \textsc{MD} are parametrized by a function called a
\emph{regularizer}. The function is assumed to be strongly convex with respect
to a given norm $\|\cdot\|$. Based on different regularizers different
algorithms with different properties can be instantiated. A summary of
algorithms for OLO is presented in Table~\ref{table:results}.  All of them,
except for \textsc{Hedge}, are instances of \textsc{FTRL} or \textsc{MD}.

Scale-free versions of \textsc{MD} include \textsc{AdaGrad
MD}~\cite{Duchi-Hazan-Singer-2011} and~\textsc{Optimistic
MD}~\cite{Rakhlin-Sridharan-2013}. However, these algorithms have non-trivial
regret bounds only when the Bregman divergence associated with the regularizer
is bounded, which happens only when the decision is bounded as well. As we show
in Section~\ref{subsection:mirror-descent-lower-bound}, this is unavoidable,
since \textsc{AdaGrad MD} and similar algorithms based on \textsc{MD} can have
$\Omega(T)$ regret if Bregman divergence is not bounded.

Only one scale-free algorithm based on \textsc{FTRL} was known. It is the
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} algorithm for
learning with expert advice, where the decision set is bounded.
Algorithms based on \textsc{FTRL} that are ``almost'' scale-free include
\textsc{FTRL-Proximal}~\cite{McMahan-Streeter-2010,McMahan-2014} and
\textsc{AdaGrad FTRL}~\cite{Duchi-Hazan-Singer-2011}.  These algorithms fail to
be scale-free due to ``off-by-one'' issue; see~\cite{McMahan-2014} and also our
discussion in Section~\ref{section:solo-ftrl}.

For unbounded decision sets no scale-free algorithm with a non-trivial regret
bound was known. Unbounded decision sets are practically important, since
learning large-scale linear models (e.g. logistic regression) can be reduced to
OLO with $\R^d$ as the decision set.
% One of our contributions is an
% \textsc{FTRL}-based scale-free algorithm with a non-trivial regret bound for
% any regularizer and any decision set, bounded or unbounded.

% While the scale-free property has been looked at in the expert setting, in
% the general OLO setting this issue has been largely ignored.  In particular,
% the \textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014}
% algorithm, for prediction with expert advice, is specifically designed to be
% scale-free.  A notable exception in the OLO literature is the discussion of
% the ``off-by-one'' issue in~\cite{McMahan-2014}, where it is explained that
% even the popular \textsc{AdaGrad} algorithm~\cite{Duchi-Hazan-Singer-2011} is
% not completely adaptive; see also our discussion in
% Section~\ref{section:solo-ftrl}. In particular, existing scale-free
% algorithms cover only some norms/regularizers and \emph{only} bounded
% decision sets. The case of \textbf{unbounded decision sets}, practically the
% most interesting one for machine learning applications, remains completely
% unsolved.

\subsection{Overview of the Results}

We design and analyze three scale-free algorithms: \textsc{AdaFTRL},
\textsc{SOLO FTRL} and \textsc{Scale-Free MD}.  \textsc{AdaFTRL} and
\textsc{SOLO FTRL} are based on \textsc{FTRL}.  \textsc{AdaFTRL} is a
generalization of
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} to arbitrary
strongly convex regularizers.  \textsc{SOLO FTRL} can be viewed as the
``correct'' scale-free version of \textsc{AdaGrad
FTRL}~\cite{Duchi-Hazan-Singer-2011} generalized to arbitrary strongly convex
regularizers.  \textsc{Scale-Free MD} is based on \textsc{MD}. It is a
generalization of \textsc{AdaGrad MD}~\cite{Duchi-Hazan-Singer-2011} to
arbitrary strongly convex regularizers, similar to \textsc{Optimistic
MD}~\cite{Rakhlin-Sridharan-2013}.  The three algorithms are presented and
analyzed in Sections~\ref{section:ada-ftrl}, \ref{section:solo-ftrl} and
\ref{section:mirror-descent} respectively.

We prove that the regret of \textsc{AdaFTRL} and \textsc{SOLO FTRL} after $T$
rounds is bounded by $O \left(\sqrt{\sup_{v \in K} f(v)
\sum_{t=1}^T\|\ell_t\|_*^2} \right)$ where $f$ is a non-negative regularizer that is
$1$-strongly convex with respect to a norm $\|\cdot\|$ and $\|\cdot\|_*$ is its
dual norm. For \textsc{Scale-Free MD}, we prove $O \left(\sqrt{\sup_{u,v \in K}
B_f(u,v) \sum_{t=1}^T\|\ell_t\|_*^2} \right)$ where $B_f$ is the Bregman
divergence associated with a $1$-strongly convex regularizer $f$. In
Section~\ref{section:lower-bound}, we show that the $\sqrt{\sum_{t=1}^T
\|\ell_t\|_*^2}$ term in the bounds term is necessary by proving
$\frac{D}{\sqrt{8}} \sqrt{\sum_{t=1}^T\|\ell_t\|_*^2}$ lower bound on the
regret of any algorithm for OLO for any decision set with diameter $D$ with
respect to the primal norm $\|\cdot\|$.

For \textsc{SOLO FTRL}, we prove that regret against a competitor $u \in K$ is
at most $O \left(f(u) \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} + \max_{t=1,2,\dots,T}
\|\ell_t\|_* \sqrt{T} \right)$.  Same as before, $f$ is a non-negative
$1$-strongly convex regularizer. The decision can be bounded or unbounded. This
is the \textbf{first adaptive algorithm for unbounded decision sets} with a
non-trivial upper bound.

All three algorithms are \textbf{any-time}, i.e., they do not need to know the
number of rounds in advance and the regret bounds hold for all time steps
simultaneously.

Our proof tecnique are also new because they rely on new homogeneous inequalities
(Lemmas~\ref{lemma:recurrence-solution}, \ref{lemma:useful},
\ref{lemma:sum-of-square-roots-inverses}).
% Previously, many proofs of regret
% bounds relied on ad-hoc non-homogeneous inequalities, or the existing
% algorithms used quantities (e.g.  $\sum_{t=1}^T \|\ell_t\|_*^2$) that are not
% available until all loss vectors are processed. Two of the inequalities
% (Lemmas~\ref{lemma:recurrence-solution} and \ref{lemma:useful}) are new and
% interesting in their own right.

Finally, in Section~\ref{subsection:mirror-descent-lower-bound}, we show
negative results for existing popular variants of \textsc{MD}. We
show that variants of \textsc{MD} have $\Omega(T)$ regret if the
Bregman divergence associated with the regularizer is unbounded.
The setting includes the standard gradient descent with step size
$1/\sqrt{t}$, \textsc{AdaGrad MD} and \textsc{Scale-Free MD} on a
unbounded decision set. This result suggests that \textsc{FTRL}
is superior to \textsc{MD}.
