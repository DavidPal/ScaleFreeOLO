\section{Introduction}
\label{section:introduction}

Online Linear Optimization (OLO) is a problem where an algorithm repeatedly
chooses a point $w_t$ from a convex decision set $K$, observes an arbitrary, or
even adversarially chosen, loss vector $\ell_t$ and suffers the loss $\langle
\ell_t, w_t \rangle$.  The goal of the algorithm is to have a small cumulative
loss. The performance of an algorithm is evaluated by the so-called regret,
which is the difference of the cumulative losses of the algorithm and of the
(hypothetical) strategy that would choose in every round the same best point in
hindsight.

OLO is a fundamental problem in machine
learning~\cite{Cesa-Bianchi-Lugosi-2006, Rakhlin-Sridharan-2009,
Shalev-Shwartz-2011}.  Many learning problems can be directly phrased as OLO,
e.g., learning with expert advice~\cite{Littlestone-Warmuth-1994, Vovk-1998,
Freund-Schapire-1997, Cesa-Bianchi-Haussler-Helmbold-Schapire-Warmuth-1997},
online combinatorial optimization~\cite{Kalai-Vempala-2005,
Helmbold-Warmuth-2009, Koolen-Warmuth-Kivinen-2010}. Other problems can be
reduced to OLO, e.g. online convex
optimization~\cite{Abernethy-Bartlett-Rakhlin-Tewari-2008},
\cite[Chapter~2]{Shalev-Shwartz-2011}, online classification
~\cite{Rosenblatt-1958, Freund-Schapire-1999} and
regression~\cite{Kivinen-Warmuth-1997},
~\cite[Chapters~11~and~12]{Cesa-Bianchi-Lugosi-2006}, multi-armed
problems~\cite[Chapter~6]{Cesa-Bianchi-Lugosi-2006},
\cite{Abernethy-Hazan-Rakhlin-2008, Bubeck-Cesa-Bianchi-2012}, and batch and
stochastic optimization of convex functions~\cite{Nemirovski-Yudin-1983,
Bubeck-2015}.  Hence, a result in OLO immediately implies other results in all
these domains.

The adversarial choice of the loss vectors received by the algorithm is what
makes the OLO problem challenging. In particular, if an OLO algorithm commits
to an upper bound on the norm of future loss vectors, its regret can be made
arbitrarily large through an adversarial strategy that produces loss vectors
with norms that exceed the upper bound.

For this reason, most of the existing OLO algorithms receive as an input---or
explicitly assume---an upper bound $B$ on the norm of the loss vectors.  The
input $B$ is often disguised as the learning rate, the regularization
parameter, or the parameter of strong convexity of the regularizer.
%Examples of
%such algorithms include \textsc{Hedge}~\cite{Freund-Schapire-1997} or online
%projected gradient descent with fixed learning rate.
However, these algorithms have two obvious drawbacks.

First, they do not come with any regret guarantee for sequences of loss vectors
with norms exceeding $B$. Second, on sequences of loss vectors with norms
bounded by $b \ll B$, these algorithms fail to have an optimal regret guarantee
that depends on $b$ rather than on $B$.

% Change spacing between rows.
\renewcommand{\arraystretch}{1.8}

\begin{table}[t]
\fontsize{8}{8.2}\selectfont
\centering
\begin{tabular}{|p{3.6cm}|c|p{3.4cm}|c|}
\hline
\textbf{Algorithm} & \textbf{Decisions Set(s)} & \textbf{Regularizer(s)} & \textbf{Scale-Free} \\ \hline \hline
\textsc{Hedge} \cite{Freund-Schapire-1997} & Probability Simplex & Negative Entropy & No \\ \hline
\textsc{GIGA} \cite{Zinkevich-2003} & Any Bounded & $\frac{1}{2}\|w\|_2^2$ & No \\ \hline
\textsc{RDA} \cite{Xiao-2010} & \textbf{Any} & \textbf{Any Strongly Convex} & No \\ \hline
\textsc{FTRL-Proximal} \cite{McMahan-Streeter-2010,McMahan-2014} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & \textbf{Yes} \\ \hline
\textsc{AdaGrad MD} \cite{Duchi-Hazan-Singer-2011} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & \textbf{Yes} \\ \hline
\textsc{AdaGrad FTRL} \cite{Duchi-Hazan-Singer-2011} & \textbf{Any} & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & No \\ \hline
\textsc{AdaHedge} \cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} & Probability Simplex & Negative Entropy & \textbf{Yes} \\ \hline
\textsc{Optimistic MD} \cite{Rakhlin-Sridharan-2013} & $\sup_{u,v \in K} \Breg_f(u,v) < \infty$ & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{NAG} \cite{Ross-Mineiro-Langford-2013} & $\{u: \max_t \langle \ell_t, u\rangle \le C\}$ & $\frac{1}{2}\|w\|_2^2 $& Partially\footnotemark \\ \hline
\textsc{Scale invariant algorithms} \cite{Orabona-Crammer-Cesa-Bianchi-2014} & \textbf{Any} & $\frac{1}{2}\|w\|_p^2 + $ any convex func. \newline $1 < p \le 2$ & Partially\textsuperscript{\ref{footnote-label}} \\ \hline
\textsc{AdaFTRL} \textbf{[this paper]} & Any Bounded & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{SOLO FTRL} \textbf{[this paper]} & \textbf{Any} & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\end{tabular}
\caption{Selected results for OLO. Best results in each column are in bold.
\label{table:results}
}
\end{table}

\footnotetext{\label{footnote-label} These algorithms attempt to produce an
invariant sequence of predictions $\langle w_t, \ell_t \rangle$, rather than a
sequence of invariant $w_t$.}

There is a clear practical need to design algorithms that adapt automatically
to the norms of the loss vectors.  A natural, yet overlooked, design method to
achieve this type of adaptivity is by insisting to have a \textbf{scale-free}
algorithm.  That is, the sequence of decisions of the algorithm does not change
if the sequence of loss vectors is multiplied by a positive constant.  The most
important property of scale-free algorithms is that both their loss and their
regret scale linearly with the maximum norm of the loss vector appearing in the
sequence.

\subsection{Previous results}

The majority of the existing algorithms for OLO are based on two generic algorithms:
\textsc{Follow The Regularizer Leader} (\textsc{FTRL}) and \textsc{Mirror
Descent} (\textsc{MD}). \textsc{FTRL} dates back to the potential-based
forecaster in \cite[Chapter~11]{Cesa-Bianchi-Lugosi-2006} and its theory was
developed in \cite{Shalev-Shwartz-2007}.  The name \textsc{Follow The
Regularized Leader} comes from~\cite{Abernethy-Hazan-Rakhlin-2008}.
Independently, the same algorithm was proposed in~\cite{Nestorov-2009} for
convex optimization under the name \textsc{Dual Averaging} and rediscovered
in~\cite{Xiao-2010} for online convex optimization. The general
version with time-varying regularizers was analyzed in
\cite{Orabona-Crammer-Cesa-Bianchi-2014}. \textsc{MD} was originally proposed
in \cite{Nemirovski-Yudin-1983} and later analyzed in~\cite{Beck-Teboulle-2003}
for convex optimization. In the online learning literature it makes his first
appearance, with a different name, in~\cite{Kivinen-Warmuth-1997}.
%A good description of \textsc{MD} is given in~\cite{Rakhlin-Sridharan-2009}.

Both \textsc{FTRL} and \textsc{MD} are parametrized by a function called 
\emph{regularizer}. The function is assumed to be strongly convex with respect
to a given norm. Based on different regularizers different algorithms with
different properties can be instantiated. A summary of algorithms for OLO is
presented in Table~\ref{table:results}.  All of them are instances of
\textsc{FTRL} or \textsc{MD}.

Scale-free versions of \textsc{MD} include \textsc{AdaGrad
MD}~\cite{Duchi-Hazan-Singer-2011} and~\textsc{Optimistic
MD}~\cite{Rakhlin-Sridharan-2013}. However, these algorithms have non-trivial
regret bounds only when the Bregman divergence associated with the regularizer
is bounded. In particular, since a bound on the Bregman divergence implies that
the decision set is bounded, the regret bounds for \textsc{AdaGrad MD} and
\textsc{Optimistic MD} are vacuous for unbounded sets. In fact, as we show in
Section~\ref{subsection:mirror-descent-lower-bound}, \textsc{AdaGrad MD} and
similar algorithms based on \textsc{MD} have $\Omega(T)$ regret if the Bregman
divergence is not bounded.

Only one scale-free algorithm based on \textsc{FTRL} was known. It is the
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} algorithm for
learning with expert advice, where the decision set is bounded.
Algorithms based on \textsc{FTRL} that are ``almost'' scale-free include
\textsc{FTRL-Proximal}~\cite{McMahan-Streeter-2010,McMahan-2014} and
\textsc{AdaGrad FTRL}~\cite{Duchi-Hazan-Singer-2011}.  These algorithms fail to
be scale-free due to ``off-by-one'' issue; see~\cite{McMahan-2014} and the
discussion in Section~\ref{section:solo-ftrl}.

For unbounded decision sets no scale-free algorithm with a non-trivial regret
bound was known. Unbounded decision sets are practically important (see e.g.
\cite{Mcmahan-Holt-Sculley-2013}), since learning of large-scale linear models
(e.g. logistic regression) is done by gradient methods that can be reduced to
OLO with decision set $\R^d$.

\subsection{Overview of the Results}

We design and analyze three scale-free algorithms: \textsc{AdaFTRL},
\textsc{SOLO FTRL} and \textsc{Scale-Free MD}.  \textsc{AdaFTRL} and
\textsc{SOLO FTRL} are based on \textsc{FTRL}.  \textsc{AdaFTRL} is a
generalization of
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} to arbitrary
strongly convex regularizers.  \textsc{SOLO FTRL} can be viewed as the
``correct'' scale-free version of \textsc{AdaGrad
FTRL}~\cite{Duchi-Hazan-Singer-2011} generalized to arbitrary strongly convex
regularizers.  \textsc{Scale-Free MD} is based on \textsc{MD}. It is a
generalization of \textsc{AdaGrad MD}~\cite{Duchi-Hazan-Singer-2011} to
arbitrary strongly convex regularizers, similar to \textsc{Optimistic
MD}~\cite{Rakhlin-Sridharan-2013}.  The three algorithms are presented in
Sections~\ref{section:ada-ftrl}, \ref{section:solo-ftrl} and
\ref{section:mirror-descent} respectively.

We prove that the regret of \textsc{AdaFTRL} and \textsc{SOLO FTRL} after $T$
rounds is bounded by $O (\sqrt{\sup_{v \in K} f(v)
\sum_{t=1}^T\|\ell_t\|_*^2} )$ where $f$ is a non-negative regularizer
that is $1$-strongly convex with respect to a norm $\|\cdot\|$ and
$\|\cdot\|_*$ is its dual norm. For \textsc{Scale-Free MD}, we prove $O
(\sqrt{\sup_{u,v \in K} B_f(u,v) \sum_{t=1}^T\|\ell_t\|_*^2} )$
where $B_f$ is the Bregman divergence associated with a $1$-strongly convex
regularizer $f$.  As we discuss, in Section~\ref{section:mirror-descent}, the
upper bound for \textsc{Scale-Free MD} is never better than the bound for bound
for \textsc{AdaFTRL} and \textsc{SOLO FTRL}.  In
Section~\ref{section:lower-bound}, we show that the $\sqrt{\sum_{t=1}^T
\|\ell_t\|_*^2}$ term in the bounds is necessary by proving $\frac{D}{\sqrt{8}}
\sqrt{\sum_{t=1}^T\|\ell_t\|_*^2}$ lower bound on the regret of any algorithm
for OLO for any decision set with diameter $D$ with respect to the primal norm
$\|\cdot\|$.

For \textsc{SOLO FTRL}, we prove that regret against a competitor $u \in K$ is
at most $O (f(u) \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} + \max_{t=1,2,\dots,T}
\|\ell_t\|_* \sqrt{T} )$.  Same as before, $f$ is a non-negative
$1$-strongly convex regularizer. This bound is non-trivial for any decision
set, bounded or unbounded.  The result makes \textsc{SOLO FTRL} the
\textbf{first adaptive algorithm for unbounded decision sets} with a
non-trivial regret bound.

All three algorithms are \textbf{any-time}, i.e., they do not need to know the
number of rounds, $T$, in advance and the regret bounds hold for all $T$
simultaneously.

Our proof techniques rely on new homogeneous
inequalities (Lemmas~\ref{lemma:recurrence-solution}, \ref{lemma:useful})
which might be of independent interest.

Finally, in Section~\ref{subsection:mirror-descent-lower-bound}, we show
negative results for existing popular variants of \textsc{MD}. We show two
examples that demonstrate that variants of \textsc{MD} can have $\Omega(T)$
regret (on sequences of loss vectors of unit norm) if the Bregman divergence
associated with the regularizer is unbounded.
%First example demonstrates
%$\Omega(T^{3/2})$ regret of the standard gradient descent with step size
%$1/\sqrt{t}$, \textsc{AdaGrad MD} and \textsc{Scale-Free MD} on a unbounded
%decision set.  Second example demonstrates $\Omega(T)$ regret for
%\textsc{Scale-Free MD} for learning with expert advice with negative entropy
%regularizer.
These results indicate that \textsc{FTRL} is superior to \textsc{MD}.
