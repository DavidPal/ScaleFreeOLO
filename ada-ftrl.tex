\section{\textsc{AdaFTRL}}
\label{section:ada-ftrl}

In this section we generalize the \textsc{AdaHedge}
algorithm~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} to the OLO setting,
showing that it retains its scale-free property. The analysis is very general
and based on general properties of strongly convex functions, rather than
specific properties of the entropic regularizer like in \textsc{AdaHedge}.

Assume that $K$ is bounded and that $R(w)$ is a strongly convex lower
semi-continuous function bounded from above.  We instantiate
Algorithm~\ref{algorithm:ftrl-varying-regularizer} with the sequence of
regularizers
\begin{equation}
\label{equation:ada-ftrl}
R_t(w) = \Delta_{t-1} R(w)
\quad \text{where}
\quad \Delta_{t}=\sum_{i=1}^{t} \Delta_{i-1} \Breg_{R^*}\left(- \frac{L_i}{\Delta_{i-1}}, -\frac{L_{i-1}}{\Delta_{i-1}}\right) \; .
\end{equation}

The sequence $\{\Delta_t\}_{t=0}^\infty$ is non-negative and non-decreasing.
Also, $\Delta_t$ as a function of $\ell_1, \ell_2, \dots, \ell_t$ is positive homogenous
of degree one, making the algorithm scale-free.

If $\Delta_{i-1} = 0$, we define $\Delta_{i-1}
\Breg_{R^*}(\frac{-L_i}{\Delta_{i-1}}, \frac{-L_{i-1}}{\Delta_{i-1}})$ as
$\lim_{a \to 0^+} a \Breg_{R^*}(\frac{-L_i}{a}, \frac{-L_{i-1}}{a})$ which
always exists and is finite; see Lemma~\ref{lemma:limit-bregman-divergence}
in~\ref{section:limits}.  Similarly, when $\Delta_{t-1} = 0$, we define $w_t =
\argmin_{w \in K} \langle L_{t-1}, w \rangle$ where ties among minimizers are
broken by taking the one with the smallest value of $R(w)$, which is unique due
to strong convexity; as we show in Lemma~\ref{lemma:prediction-limit-existence}
in~\ref{section:limits}, this is the same as $w_t = \lim_{a \to 0^+}
\argmin_{w \in K} (\langle L_{t-1}, w \rangle + aR(w))$.

Our main result is an $O(\sqrt{\sum_{t=1}^T \|\ell_t\|_*^2})$ upper bound on
the regret of the algorithm after $T$ rounds, without the need to know before
hand an upper bound on $\|\ell_t\|_*$.  We prove the theorem in
Section~\ref{section:ada-ftrl-regret-bound}.

\begin{theorem}[Regret Bound]
\label{theorem:ada-ftrl-regret-bound}
Suppose $K \subseteq V$ is a non-empty bounded closed convex subset. Let $D =
\sup_{x,y \in K} \|x - y\|$ be its diameter with respect to a norm $\|\cdot\|$.
Suppose that the regularizer $R:K \to \R$ is a non-negative lower
semi-continuous function that is $\lambda$-strongly convex with respect to
$\|\cdot\|$ and is bounded from above.  The regret of \textsc{AdaFTRL} satisfies
$$
\Regret_T(u) \le \sqrt{3} \max\left\{D, \frac{1}{\sqrt{2\lambda}} \right\} \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} \left(1 + R(u) \right) \; .
$$
\end{theorem}

The regret bound can be optimized by choosing the optimal multiple of the
regularizer.  Namely, we choose regularizer of the form $\lambda f(w)$ where
$f(w)$ is $1$-strongly convex and optimize over $\lambda$. The result of the
optimization is the following corollary.  Its proof can be found in~\ref{section:ada-ftrl-proofs}.

\begin{corollary}[Regret Bound]
\label{corollary:ada-ftrl-regret-bound}
Suppose $K \subseteq V$ is a non-empty bounded closed convex subset. Suppose
$f:K \to \R$ is a non-negative lower semi-continuous function that is
$1$-strongly convex with respect to $\|\cdot\|$ and is bounded from above.  The
regret of \textsc{AdaFTRL} with regularizer
$$
R(w) = \frac{f(w)}{16 \cdot \sup_{v \in K} f(v)}
\qquad \text{satisfies} \qquad
\Regret_T \le
5.3 \sqrt{\sup_{v \in K} f(v) \sum_{t=1}^T \|\ell_t\|_*^2} \; .
$$
\end{corollary}

\subsection{Proof of Regret Bound for \textsc{AdaFTRL}}
\label{section:ada-ftrl-regret-bound}

\begin{lemma}[Initial Regret Bound]
\label{lemma:initial-regret-bound}
\textsc{AdaFTRL}, for any $u \in K$ and any $u \ge 0$, satisfies
$\Regret_T(u) \le \left(1 + R(u) \right) \Delta_T$.
\end{lemma}

\begin{proof}
Let $R_t(w) = \Delta_{t-1} R(w)$. Since $R$ is non-negative,
$\{R_t\}_{t=1}^\infty$ is non-decreasing.  Hence, $R_t^*(\ell) \ge
R_{t+1}^*(\ell)$ for every $\ell \in V^*$ and thus $R_t^*(-L_t) -
R_{t+1}^*(-L_t) \ge 0$.  So, by Lemma~\ref{lemma:generic-regret-bound},
\begin{equation}
\label{equation:regret-bound-inequality}
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^{T} \Breg_{R_t^*}(-L_t, -L_{t-1}) \; .
\end{equation}
Technically, \eqref{equation:regret-bound-inequality} is not justified since $R_t$
might not be strongly convex. This happens when $\Delta_{t-1} = 0$. In order to justify
\eqref{equation:regret-bound-inequality}, we consider a different algorithm
that initializes $\Delta_0 = \epsilon$ where $\epsilon > 0$; that ensures that $\Delta_{t-1} > 0$ and $R_t$ is strongly convex.
Applying  Lemma~\ref{lemma:generic-regret-bound} and then taking limit $\epsilon \to 0$,
yields \eqref{equation:regret-bound-inequality}.

Since, $\Breg_{R_t^*}(u,v) = \Delta_{t-1} \Breg_{R^*}(\frac{u}{\Delta_{t-1}},
\frac{v}{\Delta_{t-1}})$ by definition of Bregman divergence and Part 8 of
Proposition~\ref{proposition:conjugate-properties}, we have $\sum_{t=1}^T
\Breg_{R_t^*}(-L_t, -L_{t-1}) = \Delta_T$.
\end{proof}

\begin{lemma}[Recurrence]
\label{lemma:gap-recurrence}
Let $D = \sup_{u, v \in K} \|u -v\|$ be the diameter of $K$.
The sequence $\{\Delta_t\}_{t=1}^\infty$ generated by \textsc{AdaFTRL} satisfies for any $t \ge 1$,
$$
\Delta_t \le \Delta_{t-1} + \min \left\{ D\|\ell_t\|_*, \ \frac{\|\ell_t\|_*^2}{2\lambda \Delta_{t-1}} \right\} \; .
$$
\end{lemma}

\begin{proof}
The inequality results from strong convexity of $R_t(w)$ and
Proposition~\ref{proposition:conjugate-properties}.
\end{proof}

\begin{lemma}[Solution of the Recurrence]
\label{lemma:recurrence-solution}
Let $D$ be the diameter of $K$. The sequence $\{\Delta_t\}_{t=0}^\infty$
generated by \textsc{AdaFTRL} satisfies for any $T \ge 0$,
$$
\Delta_T \le \sqrt{3} \max\left\{D, \frac{1}{\sqrt{2\lambda}} \right\} \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} \; .
$$
\end{lemma}
%
Proof of the Lemma~\ref{lemma:recurrence-solution} is deferred to~\ref{section:ada-ftrl-proofs}.
Theorem~\ref{theorem:ada-ftrl-regret-bound} follows from
Lemmas~\ref{lemma:initial-regret-bound}~and~\ref{lemma:recurrence-solution}.
