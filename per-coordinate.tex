\subsection{Per-Coordinate Learning}

An interesting class of algorithms proposed in~\cite{McMahan-Streeter-2010} and
\cite{Duchi-Hazan-Singer-2011} are based on the so-called per-coordinate
learning rates.  As shown in \cite{Streeter-McMahan-2010}, any algorithm for
OLO can be used with per-coordinate learning rates as well.

Abstractly, we assume that the decision set is a Cartesian product $K=K_1
\times K_2 \times \dots \times K_d$ of a finite number of convex sets.  On each
factor $K_j$, $j=1,2,\dots,d$, we can run any OLO algorithm separately and we
denote by $\Regret_T^{(j)}(u_j)$ its regret with respect to $u_j \in K_j$. The
overall regret with respect to any $u=(u_1, u_2, \dots, u_d) \in K$ can be
written as
$$
\Regret_T(u) = \sum_{j=1}^d \Regret_T^{(j)}(u_j) \; .
$$

If the algorithm for each factor is scale-free, the overall algorithm is
clearly scale-free as well. Hence, even if not explicitly mentioned in the
text, any algorithm we present can be trivially transformed to a per-coordinate
version.
