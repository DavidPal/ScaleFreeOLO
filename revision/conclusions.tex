\section{Conclusions}
\label{section:conclusions}

We have investigated scale-free algorithms for online linear optimization and
we have shown that the scale-free property leads to algorithms which have optimal
regret and do not need to know or assume \textbf{anything} about the sequence
of loss vectors. In particular, the algorithms do not assume any upper or lower
bounds on the norms of the loss vectors or the number of rounds.

We have designed two scale-free algorithms based on \textsc{Follow The
Regularizer Leader}.  Their regret is $O \left(\sqrt{\sup_{v \in K} f(v)
\sum_{t=1}^T \|\ell\|_*^2} \right)$ where $f$ is any non-negative $1$-strongly
convex function with respect to a norm $\|\cdot\|$ defined on the decision set
and where $\|\cdot\|_*$ is the dual norm to $\|\cdot\|$.

A similar, but weaker result holds for a scale-free algorithm based on
\textsc{Mirror Descent}. However, we have also shown this algorithm to be strictly
weaker than algorithms based on \textsc{Follow The Regularizer Leader}. Namely,
we gave examples of regularizers for which the scale-free version of
\textsc{Mirror Descent} has $\Omega(T)$ regret or worse.

We have proved an $\frac{D}{\sqrt{8}} \sqrt{\sum_{t=1}^T \|\ell\|_*^2}$
lower bound on the regret of any algorithm for any decision set with diameter $D$.

We have shown that the scale-free \textsc{FTRL}-based algorithm \textsc{SOLO FTRL} has
regret $O \left(f(u) \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} + \max_{t=1,2,\dots,T}
\|\ell_t\|_* \sqrt{T} \right)$ with respect to any competitor $u \in K$, where
$f$ is any non-negative $1$-strongly convex function defined on the decision
set.  The latter result makes sense even when the decision set $K$ is
unbounded. Notice that with the regularizer $f(u) = \frac{1}{2}\|u\|_2^2$ the
regret depends quadratically on the norm of the competitor $\|u\|_2$. There
exist non-scale-free algorithms \cite{McMahan-Streeter-2012,
McMahan-Abernethy-2013, Orabona-2013, McMahan-Orabona-2014, Orabona-2014} that
have only a $O(\|u\|_2 \sqrt{\log \|u\|_2})$ or $O(\|u\|_2 \log \|u\|_2)$
dependency.  These algorithms assume an a priori bound on the norm of the loss
vectors.  This leads to another open problem:
%
\begin{quotation}
\noindent
\emph{Is there a scale-free algorithm for the decision set $\R^d$, the regret of
which has $O(\|u\|_2 \sqrt{\log \|u\|_2})$ dependency on the competitor $u$?}
\end{quotation}
