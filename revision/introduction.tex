\section{Introduction}
\label{section:introduction}

Online Linear Optimization (OLO) is a problem where an algorithm repeatedly
chooses a point $w_t$ from a convex decision set $K$, observes an arbitrary, or
even adversarially chosen, loss vector $\ell_t$ and suffers the loss $\langle
\ell_t, w_t \rangle$.  The goal of the algorithm is to have a small cumulative
loss. The performance of an algorithm is evaluated by the so-called regret,
which is the difference of the cumulative losses of the algorithm and of the
(hypothetical) strategy that would choose in every round the same best point in
hindsight.

OLO is a fundamental problem in machine
learning~\cite{Cesa-Bianchi-Lugosi-2006, Rakhlin-Sridharan-2009,
Shalev-Shwartz-2011}.  Many learning problems can be directly phrased as OLO,
e.g., learning with expert advice~\cite{Littlestone-Warmuth-1994, Vovk-1998,
Freund-Schapire-1997, Cesa-Bianchi-Haussler-Helmbold-Schapire-Warmuth-1997}
and online combinatorial optimization~\cite{Kalai-Vempala-2005,
Helmbold-Warmuth-2009, Koolen-Warmuth-Kivinen-2010}. Other problems can be
reduced to OLO, e.g., online convex
optimization~\cite{Abernethy-Bartlett-Rakhlin-Tewari-2008},
\cite[Chapter~2]{Shalev-Shwartz-2011}, online classification
~\cite{Rosenblatt-1958, Freund-Schapire-1999} and
regression~\cite{Kivinen-Warmuth-1997},
~\cite[Chapters~11~and~12]{Cesa-Bianchi-Lugosi-2006}, multi-armed bandits
problems~\cite[Chapter~6]{Cesa-Bianchi-Lugosi-2006},
\cite{Abernethy-Hazan-Rakhlin-2008, Bubeck-Cesa-Bianchi-2012}, and batch and
stochastic optimization of convex functions~\cite{Nemirovski-Yudin-1983,
Bubeck-2015}.  Hence, a result in OLO immediately implies other results in all
these domains.

The adversarial choice of the loss vectors received by the algorithm is what
makes the OLO problem challenging. In particular, if an OLO algorithm commits
to an upper bound on the norm of future loss vectors, its regret can be made
arbitrarily large through an adversarial strategy that produces loss vectors
with norms that exceed the upper bound.

For this reason, most of the existing OLO algorithms receive as an input---or
implicitly assume---an upper bound $B$ on the norm of the loss vectors.  The
input $B$ is often disguised as the learning rate, the regularization
parameter, or the parameter of strong convexity of the regularizer.
However, these algorithms have two obvious drawbacks.

First, they do not come with any regret guarantee for sequences of loss vectors
with norms exceeding $B$. Second, on sequences of loss vectors with norms
bounded by $b \ll B$, these algorithms fail to have an optimal regret guarantee
that depends on $b$ rather than on $B$.

% Change spacing between rows.
\renewcommand{\arraystretch}{1.8}

\begin{table}[t]
\fontsize{8}{8.2}\selectfont
\centering
\begin{tabular}{|p{3.6cm}|c|p{3.4cm}|c|}
\hline
\textbf{Algorithm} & \textbf{Decisions Set(s)} & \textbf{Regularizer(s)} & \textbf{Scale-Free} \\ \hline \hline
\textsc{Hedge} \cite{Freund-Schapire-1997} & Probability Simplex & Negative Entropy & No \\ \hline
\textsc{GIGA} \cite{Zinkevich-2003} & Any Bounded & $\frac{1}{2}\|w\|_2^2$ & No \\ \hline
\textsc{RDA} \cite{Xiao-2010} & \textbf{Any} & \textbf{Any Strongly Convex} & No \\ \hline
\textsc{FTRL-Proximal} \cite{McMahan-Streeter-2010,McMahan-2014} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func.\footnotemark & \textbf{Yes} \\ \hline
\textsc{AdaGrad MD} \cite{Duchi-Hazan-Singer-2011} & Any Bounded & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & \textbf{Yes} \\ \hline
\textsc{AdaGrad FTRL} \cite{Duchi-Hazan-Singer-2011} & \textbf{Any} & $\frac{1}{2}\|w\|_2^2 + $ any convex func. & No \\ \hline
\textsc{AdaHedge} \cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} & Probability Simplex & Negative Entropy & \textbf{Yes} \\ \hline
\textsc{NAG} \cite{Ross-Mineiro-Langford-2013} & $\{u: \max_t \langle \ell_t, u\rangle \le C\}$ & $\frac{1}{2}\|w\|_2^2 $& Partially\footnotemark \\ \hline
\textsc{Scale invariant algorithms} \cite{Orabona-Crammer-Cesa-Bianchi-2014} & \textbf{Any} & $\frac{1}{2}\|w\|_p^2 + $ any convex func. \newline $1 < p \le 2$ & Partially\textsuperscript{\ref{footnote-label2}} \\ \hline
%\textsc{AdaFTRL} \textbf{[this paper]} & Any Bounded & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{Scale-free MD} \textbf{[this paper]} & $\sup_{u,v \in K} \Breg_f(u,v) < \infty$ & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\textsc{SOLO FTRL} \textbf{[this paper]} & \textbf{Any} & \textbf{Any Strongly Convex} & \textbf{Yes} \\ \hline
\end{tabular}
\caption{Selected results for OLO. Best results in each column are in bold.
\label{table:results}
}
\end{table}

\addtocounter{footnote}{-2}

\stepcounter{footnote}\footnotetext{\label{footnote-label1} Even if, in
principle the FTRL-Proximal algorithm can be used with any proximal regularizer,
to the best of our knowledge a general way to construct proximal regularizers is
not known. The only proximal regularizer we are aware is based on the 2-norm.}

\stepcounter{footnote}\footnotetext{\label{footnote-label2} These algorithms
attempt to produce an invariant sequence of predictions $\langle w_t, \ell_t
\rangle$, rather than a sequence of invariant $w_t$.}

There is a clear practical need to design algorithms that adapt automatically to
the norms of the loss vectors.  A natural, yet overlooked, design method to
achieve this type of adaptivity is by insisting to have a \textbf{scale-free}
algorithm.  That is, with the same parameters, the sequence of decisions of the
algorithm does not change if the sequence of loss vectors is multiplied by a
positive constant.  The most important property of scale-free algorithms is that
both their loss and their regret scale linearly with the maximum norm of the
loss vector appearing in the sequence.

\subsection{Previous results}

The majority of the existing algorithms for OLO are based on two generic
algorithms: \textsc{Follow The Regularizer Leader} (\textsc{FTRL}) and
\textsc{Mirror Descent} (\textsc{MD}). \textsc{FTRL} dates back to the
potential-based forecaster in \cite[Chapter~11]{Cesa-Bianchi-Lugosi-2006} and
its theory was developed in \cite{Shalev-Shwartz-2007}.  The name \textsc{Follow
The Regularized Leader} comes from~\cite{Abernethy-Hazan-Rakhlin-2008}.
Independently, the same algorithm was proposed in~\cite{Nestorov-2009} for
convex optimization under the name \textsc{Dual Averaging} and rediscovered
in~\cite{Xiao-2010} for online convex optimization. Time-varying regularizers
were analyzed in~\cite{Duchi-Hazan-Singer-2011} and generalized in
\cite{Orabona-Crammer-Cesa-Bianchi-2014}. \textsc{MD} was originally proposed in
\cite{Nemirovski-Yudin-1983} and later analyzed in~\cite{Beck-Teboulle-2003} for
convex optimization. In the online learning literature it makes its first
appearance, with a different name, in~\cite{Kivinen-Warmuth-1997}.

Both \textsc{FTRL} and \textsc{MD} are parametrized by a function called
\emph{regularizer}. The function is assumed to be strongly convex with respect
to a given norm. Based on different regularizers different algorithms with
different properties can be instantiated. A summary of algorithms for OLO is
presented in Table~\ref{table:results}.  All of them are instances of
\textsc{FTRL} or \textsc{MD}.

Scale-free versions of \textsc{MD} include \textsc{AdaGrad
MD}~\cite{Duchi-Hazan-Singer-2011}. However, the AdaGrad MD algorithm has a
non-trivial regret bounds only when the Bregman divergence associated with the
regularizer is bounded. In particular, since a bound on the Bregman divergence
implies that the decision set is bounded, the regret bound for \textsc{AdaGrad
MD} and is vacuous for unbounded sets. In fact, as we show in
Section~\ref{subsection:mirror-descent-lower-bound}, \textsc{AdaGrad MD} and
similar algorithms based on \textsc{MD} incurs $\Omega(T)$ regret, in the worst
case, if the Bregman divergence is not bounded.

Only one scale-free algorithm based on \textsc{FTRL} was known. It is the
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} algorithm for
learning with expert advice, where the decision set is bounded. An algorithm
based on \textsc{FTRL} that is ``almost'' scale-free is \textsc{AdaGrad
FTRL}~\cite{Duchi-Hazan-Singer-2011}.  This algorithm fail to be scale-free due
to ``off-by-one'' issue; see~\cite{McMahan-2014} and the discussion in
Section~\ref{section:solo-ftrl}. Instead,
\textsc{FTRL-Proximal}~\cite{McMahan-Streeter-2010,McMahan-2014} solves the
off-by-one issue, but it requires proximal regularizers, that in general do not
have a simple form and even in the simple 2-norm case requires bounded domains.

For unbounded decision sets no scale-free algorithm with a non-trivial regret
bound was known. Unbounded decision sets are practically important (see, e.g.,
\cite{Mcmahan-Holt-Sculley-2013}), since learning of large-scale linear models
(e.g., logistic regression) is done by gradient methods that can be reduced to
OLO with decision set $\R^d$.

\subsection{Overview of the Results}

We design and analyze two scale-free algorithms: \textsc{SOLO FTRL} and
\textsc{Scale-Free MD}.  A third one, \textsc{AdaFTRL}, is presented in
the Appendix. \textsc{SOLO FTRL} and \textsc{AdaFTRL} are based on
\textsc{FTRL}.  \textsc{AdaFTRL} is a generalization of
\textsc{AdaHedge}~\cite{de-Rooij-van-Erven-Grunwald-Koolen-2014} to arbitrary
strongly convex regularizers.  \textsc{SOLO FTRL} can be viewed as the
``correct'' scale-free version of the diagonal version of \textsc{AdaGrad
FTRL}~\cite{Duchi-Hazan-Singer-2011} generalized to arbitrary strongly convex
regularizers.  \textsc{Scale-Free MD} is based on \textsc{MD}. It is a
generalization of \textsc{AdaGrad MD}~\cite{Duchi-Hazan-Singer-2011} to
arbitrary strongly convex regularizers.  The three algorithms are presented in
Sections~\ref{section:solo-ftrl} and \ref{section:mirror-descent}, and
\ref{section:ada-ftrl}, respectively.

We prove that the regret of \textsc{SOLO FTRL} and \textsc{AdaFTRL} after $T$
rounds is bounded by $O (\sqrt{\sup_{v \in K} f(v) \sum_{t=1}^T\|\ell_t\|_*^2} )$
where $f$ is a non-negative regularizer that is $1$-strongly convex with respect
to a norm $\|\cdot\|$ and $\|\cdot\|_*$ is its dual norm. For \textsc{Scale-Free
MD}, we prove $O (\sqrt{\sup_{u,v \in K} B_f(u,v) \sum_{t=1}^T\|\ell_t\|_*^2} )$
where $B_f$ is the Bregman divergence associated with a $1$-strongly convex
regularizer $f$. In Section~\ref{section:lower-bound}, we show that the
$\sqrt{\sum_{t=1}^T \|\ell_t\|_*^2}$ term in the bounds is necessary by proving
a $\frac{D}{\sqrt{8}} \sqrt{\sum_{t=1}^T\|\ell_t\|_*^2}$ lower bound on the
regret of any algorithm for OLO for any decision set with diameter $D$ with
respect to the primal norm $\|\cdot\|$.

For \textsc{SOLO FTRL}, we prove that the regret against a competitor $u \in K$
is at most $O (f(u) \sqrt{\sum_{t=1}^T \|\ell_t\|_*^2} + \max_{t=1,2,\dots,T}
\|\ell_t\|_* \sqrt{T} )$.  As before, $f$ is a non-negative $1$-strongly convex
regularizer. This bound is non-trivial for any decision set, bounded or
unbounded.  The result makes \textsc{SOLO FTRL} the \textbf{first adaptive
algorithm for unbounded decision sets} with a non-trivial regret bound.

All three algorithms are \textbf{any-time}, i.e., they do not need to know the
number of rounds, $T$, in advance and the regret bounds hold for all $T$
simultaneously.

Our proof techniques rely on new homogeneous
inequalities (Lemmas~\ref{lemma:useful}, \ref{lemma:recurrence-solution})
which might be of independent interest.

Finally, in Section~\ref{subsection:mirror-descent-lower-bound}, we show
negative results for existing popular variants of \textsc{MD}. We show two
examples that demonstrate that variants of \textsc{MD} can have $\Omega(T)$
regret (on sequences of loss vectors of unit norm) if the Bregman divergence
associated with the regularizer is unbounded. These results indicate that
\textsc{FTRL} is superior to \textsc{MD}.
